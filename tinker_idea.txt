直接强化学习在成对判断上

在基于人类反馈的强化学习（RLHF）中，我们使用由人类收集的成对偏好数据集来训练奖励模型。这是必要的，因为出于成本和实际操作的考虑，我们不希望在强化学习训练过程中直接询问人类用户。
《宪法人工智能》论文提出了基于人工智能反馈的强化学习（RLAIF）。作者首先利用提示模型进行成对比较来创建偏好数据，然后将这些数据与人工收集的偏好数据混合，最后对生成的偏好模型进行强化学习。然而，既然要使用人工智能反馈，那么为何还要额外训练一个偏好模型呢？如果我们在强化学习过程中直接询问提示法官的意见岂不是更好？
Lee 等人的论文提出了一种方法，即直接在提示式评测器上进行训练，他们称之为直接 RLAIF。然而，他们没有使用成对比较，而是让评测器输出一个标量分数。
在强化学习过程中，直接进行成对比较是很自然的做法，可以使用评判器来对成对匹配进行评分。我们在Tinker Cookbook中实现了这一点：给定一个提示，抽取 G 个补全样本，然后对不同的补全结果进行 G^2-G（顺序敏感）匹配，最后根据胜率定义奖励。
在当前代码中，我们使用了一个基于现有偏好数据集训练的成对偏好模型。然而，我们也可以使用提示式评判模型来进行比较。
这项任务是比较 RLAIF 的直接方法和间接方法。在间接方法中，我们使用提示式评测员创建偏好数据集，然后基于该数据集训练偏好模型，最后对该偏好模型进行强化学习。在直接方法中，我们在强化学习过程中直接向提示式评测员提问。
为了进行量化比较，可以使用指令后评估。您还可以使用比训练模型更大的评判模型——这样，即使策略能够破解小型评判模型，大型评判模型仍然可以提供可靠的信号。此外，您还需要在训练后对样本进行定性评估——这通常在非验证环境下进行强化学习时是必要的，因为在这种情况下，自动化指标很难全面反映情况。



GAN训练用于笑话生成

在某些情况下，训练一个好的奖励模型或评判标准很困难，而收集一组高质量的演示（由人工创建和审核）则相对容易。但通常情况下，不可能收集到足够大的监督数据集来训练模型到所需的质量水平。
另一种训练方法是“生成对抗网络”（GAN）方法，该方法在判别器和生成器之间进行极小极大优化（零和博弈）。生成器和判别器都可以是推理模型，并使用强化学习（RL）进行训练。
给定一个笑话数据集，将其转换为一个条件笑话生成数据集，其中每个笑话都与一个合成指令相关联，例如“生成一个关于……的、暗示……的讲故事的笑话”。通过使其成为条件性的，我们部分解决了如何确保生成器输出多样化的多样性问题；并且我们也使最终产物更有用（因为它将可控）。
与标准 GAN 训练类似，生成器和判别器之间需要进行自博弈优化。生成器生成一个笑话，判别器判断它是真实的还是生成的。判别器可以输出二元决策或连续决策，后者可以通过适当的评分规则进行优化，以确保其输出的概率经过校准。判别器输出经过校准的概率值，很可能为生成器提供更有价值的训练信号。



强化学习记忆信息能力的实证研究

LoRA 无悔论文对监督学习和强化学习获取信息的速度提出了一些理论论证。这些论证依赖于各种假设，而这些假设在实际实验环境中的适用性尚存争议。
然而，我们也可以做一些实验来验证这个理论。特别是，如果一个算法能够以一定的速率学习新信息，那么它应该能够以同样的速率学习“随机”信息——例如，在[1, 2, ..., N]范围内均匀分布的随机整数，其熵为log(N)。
创建一个包含潜在随机数的环境，策略必须记住这个随机数才能最大化奖励。使用二元奖励或连续奖励，策略需要多少轮才能记住这个随机数？这与信息论的论证有何关联？
超越这个案例，尝试建立一个记忆测试平台，并比较监督学习、带有回合结束奖励的强化学习和带有每步奖励的强化学习的信息吸收率。



将《吵闹的学生》改编成法学硕士（LLM）作品

在机器学习的早期阶段，噪声学生自蒸馏是一种利用大型无标签数据集的常用技术。一些近期的研究探索了“自训练”或从自身生成的奖励信号中学习，但据我们所知，目前还没有人尝试将早期的半监督学习技术应用于学习型学习模型（LLM）的场景。例如，这里有一个针对强化学习可验证奖励（ RLVR）场景的方案：我们有一个包含参考答案的小型训练集和一个更大的无标签数据集。我们能否利用训练集进行引导，从而有效地利用这个更大的无标签数据集？
1. 在训练集上运行强化学习，
2. 使用所得模型，通过对学生输出结果的共识，对未标记的数据集进行标记。
3. 在新标注的更大数据集上运行强化学习程序
4. 使用生成的模型对整个数据集进行重新标记。
5. 再次运行RL……
最初的噪声学生算法通过向学生模型添加噪声（例如，dropout）来增加任务难度。但对于上述过程而言，这可能并非必要，因为学生模型本身已经通过采样引入了噪声（而目标模型是通过共识机制获得的，这降低了噪声）。
作为上述过程的变体，也可以用监督学习（蒸馏）替换一个或多个强化学习步骤。最近有几篇相关的论文探讨了大型推理模型/强化学习虚拟现实中的自训练思想，并使用多数投票作为强化学习信号：《大型推理模型能否自训练？》 和《测试时间强化学习》。



政策背景提炼
上下文蒸馏训练一个学生模型（上下文为空）来匹配一个教师模型（上下文较长且详细，例如，包含少量样本）。通常，学生模型和教师模型源自同一个基础模型，并且学生模型使用教师模型的权重进行初始化。参见Anthropic (2021)和Snell 等人 (2022)。
先前的上下文蒸馏工作采用离策略蒸馏——即，我们从提示教师那里收集样本数据集，然后使用监督学习对学生进行数据训练。
另一种方法是使用策略内蒸馏，如我们的博客文章中所述，该方法基于Agarwal 等人 (2023)和Gu 等人 (2023)的早期工作。
为了比较这些方法，您可以使用少样本学习设置，例如，多样本情境学习中的任务。比较：
* 仅非策略性蒸馏
* 仅政策提炼
* 先进行非政策性提炼，再进行政策性提炼


复制宪法人工智能，包括使用基础模型

尽管基于人工智能反馈的强化学习已被广 泛应用，但人们大多依赖于现有的、经过指令调整的模型。这导致对数据生成模型的强烈隐性依赖，并且很难区分哪些结果源自宪法，哪些源自解释宪法的数据生成模型。
这篇论文最初写于LLM时代早期，它提供了一种从头开始的训练流程，该流程从基础模型出发，并且不使用经过指令调整的模型作为流程的一部分。在论文中，作者通过微调基础模型来训练一个仅包含有用信息的模型，并使用少样本提示进行成对比较。
这种复制方法对于研究模型结构以及其他后续训练研究都很有用，在这些研究中，我们希望探索模型设计的各个方面，同时避免受到现有辅助模型的“污染”。此外，由此产生的模型可能在某些方面更接近基础模型，例如，它们可能更擅长以不同的风格写作，而这些风格是现有基于指令调整的模型难以驾驭的。



复制开放式角色训练

最近一篇题为《开放式角色训练：通过宪法人工智能塑造人工智能助手的个性》的论文描述了一种微调模型以使其具有特定个性和性格特征的方法，这很可能受到了 Anthropic 公司用来塑造 Claude 角色的技术的启发，正如这篇博客文章中所描述的那样。
作者提出了一种方案，并将其应用于针对不同人群的11份不同的手写宪法。他们首先使用一个经过调整的指令模型，然后应用以下步骤：
* DPO 对正对 := 具有结构特征的强模型，负对 := 不具有结构特征的弱模型。
* 生成自我反思和自我互动数据。然后基于该数据集进行监督式微调。这一步骤是一种提示提炼，因为这些数据是在系统提示下生成的，而学生是在没有这些提示的情况下接受训练的。
然后，作者评估了这些字符及其鲁棒性，以及它们在其他基准测试中的表现。
以下是一个用于复现该论文的建议方案：
* 复制训练流程，并将其应用于几个提供的宪法，对 Tinker 中托管的其中一个模型进行微调。
* 在一组固定的提示中（包括一些与性格特征相关的提示和一些与性格特征无关的提示），从所有这些精细调整的模型中抽取样本，并定性分析差异。
* 实施论文中的一种定量评价方法，以确定模型的特征。
以下是一些超越纸面研究、利用廷克方法优势的方法：
* 该论文使用了规模高达 80 亿的模型。您可以使用 Tinker 提供的更大规模的模型应用相同的方法。您还可以观察行为和指标如何随模型规模变化。
* 创建你自己的宪法——你能创造出最有趣的角色是什么？
* 尝试使用策略梯度强化学习 (RL) 来训练偏好模型，而不是使用 DPO。请参阅Tinker Cookbook 中的 RLHF 示例，了解如何通过对一组样本进行匹配来训练成对奖励。以下是定义偏好模型的几种方法：
    * 使用提示式评判器（即未经微调的评判器）。为了定义评判器，可以使用一个经过严格指令调整的模型，将宪法置于特定语境中，并要求该模型审查两个答案，判断哪个答案更符合宪法。
    * 首先收集一组配对数据集，然后基于这些配对训练偏好模型。您可以将基于性格的偏好数据与另一个基于实用性的偏好数据集混合使用。


