{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCLcAmON-m2i"
      },
      "source": [
        "# Tensor2Tensor Reinforcement Learning\n",
        "\n",
        "The `rl` package provides the ability to run model-free and model-based reinforcement learning algorithms.\n",
        "\n",
        "Currently, we support the Proximal Policy Optimization ([PPO](https://arxiv.org/abs/1707.06347)) and Simulated Policy Learning ([SimPLe](https://arxiv.org/abs/1903.00374)).\n",
        "\n",
        "Below you will find examples of PPO training using `trainer_model_free.py` and SimPLe traning using `trainer_model_based.py`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RW7gEGp3e87G",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Copyright 2018 Google LLC.\n",
        "\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pq0BqXm4-3gJ",
        "outputId": "dc6d365a-0f38-4d4e-9164-a43eade90666",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.13.1 (from versions: 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.18.1, 2.19.0rc0, 2.19.0, 2.19.1, 2.20.0rc0, 2.20.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==1.13.1\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q tensorflow==1.13.1\n",
        "!pip install -q tensorflow_probability==0.6.0\n",
        "!pip install -q tensor2tensor==1.13.1\n",
        "!pip install -q gym[atari]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7-Ni-39DGZW"
      },
      "outputs": [],
      "source": [
        "# Helper function for playing videos in the colab.\n",
        "def play_video(path):\n",
        "  from IPython.core.magics.display import HTML\n",
        "  display_path = \"/nbextensions/vid.mp4\"\n",
        "  display_abs_path = \"/usr/local/share/jupyter\" + display_path\n",
        "  !rm -f $display_abs_path\n",
        "  !ffmpeg -loglevel error -i $path $display_abs_path\n",
        "  return HTML(\"\"\"\n",
        "    <video width=\"640\" height=\"480\" controls>\n",
        "      <source src=\"{}\" type=\"video/mp4\">\n",
        "    </video>\n",
        "  \"\"\".format(display_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pueuiKUmAOUT"
      },
      "source": [
        "# Play using a pre-trained policy\n",
        "\n",
        "We provide pretrained policies for the following games from the Atari Learning Environment ( [ALE](https://github.com/mgbellemare/Arcade-Learning-Environment)) : alien,\n",
        "amidar,\n",
        " assault,\n",
        " asterix,\n",
        " asteroids,\n",
        " atlantis,\n",
        " bank_heist,\n",
        " battle_zone,\n",
        " beam_rider,\n",
        " bowling,\n",
        " boxing,\n",
        " breakout,\n",
        " chopper_command,\n",
        " crazy_climber,\n",
        " demon_attack,\n",
        " fishing_derby,\n",
        " freeway,\n",
        " frostbite,\n",
        " gopher,\n",
        " gravitar,\n",
        " hero,\n",
        " ice_hockey,\n",
        " jamesbond,\n",
        " kangaroo,\n",
        " krull,\n",
        " kung_fu_master,\n",
        " ms_pacman,\n",
        " name_this_game,\n",
        " pong,\n",
        " private_eye,\n",
        " qbert,\n",
        " riverraid,\n",
        " road_runner,\n",
        " seaquest,\n",
        " up_n_down,\n",
        " yars_revenge.\n",
        "\n",
        " We have 5 checkpoints for each game saved on Google Storage. Run the following command get the storage path:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9pKfNbDFfVh"
      },
      "outputs": [],
      "source": [
        "# experiment_id is an integer from [0, 4].\n",
        "def get_run_dir(game, experiment_id):\n",
        "  from tensor2tensor.data_generators.gym_env import ATARI_GAMES_WITH_HUMAN_SCORE_NICE\n",
        "  EXPERIMENTS_PER_GAME = 5\n",
        "  run_id = ATARI_GAMES_WITH_HUMAN_SCORE_NICE.index(game) * EXPERIMENTS_PER_GAME + experiment_id + 1\n",
        "  return \"gs://tensor2tensor-checkpoints/modelrl_experiments/train_sd/{}\".format(run_id)\n",
        "\n",
        "get_run_dir('pong', 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77fFdm-cFEOB"
      },
      "source": [
        "To evaluate and generate videos for a pretrained policy on Pong:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-nGlbuTAQXj",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "game = 'pong'\n",
        "run_dir = get_run_dir(game, 1)\n",
        "!python -m tensor2tensor.rl.evaluator \\\n",
        "  --loop_hparams_set=rlmb_long_stochastic_discrete \\\n",
        "  --loop_hparams=game=$game,eval_max_num_noops=8,eval_sampling_temps=[0.5] \\\n",
        "  --policy_dir=$run_dir/policy \\\n",
        "  --eval_metrics_dir=pong_pretrained \\\n",
        "  --debug_video_path=pong_pretrained \\\n",
        "  --num_debug_videos=4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKWPdwP8BW_v"
      },
      "source": [
        "The above command will run a single evaluation setting to get the results fast. We usually run a grid of different settings (sampling temperatures and whether to do initial no-ops). To do that, remove `eval_max_num_noops=8,eval_sampling_temps=[0.5]` from the command. You can override the evaluation settings:\n",
        "\n",
        "```\n",
        "  --loop_hparams=game=pong,eval_max_num_noops=0,eval_sampling_temps=[0.0]\n",
        " ```\n",
        "\n",
        " The evaluator generates videos from the environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "At9LC5rxFyv2"
      },
      "outputs": [],
      "source": [
        "play_video('pong_pretrained/0.avi')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-SyGcZBCmPn"
      },
      "source": [
        "# Train your policy (model-free training)\n",
        "Training model-free on Pong (it takes a few hours):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIQazd5aCocc"
      },
      "outputs": [],
      "source": [
        "!python -m tensor2tensor.rl.trainer_model_free \\\n",
        "  --hparams_set=rlmf_base \\\n",
        "  --hparams=game=pong \\\n",
        "  --output_dir=mf_pong"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbSjwVAtCvLY"
      },
      "source": [
        "Hyperparameter sets are defined in `tensor2tensor/models/research/rl.py`. You can override them using the hparams flag, e.g.\n",
        "\n",
        "```\n",
        "--hparams=game=kung_fu_master,frame_stack_size=5\n",
        "```\n",
        "\n",
        "As in model-based training, the periodic evaluation runs with timestep limit of 1000. To do full evaluation after training, run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jppi4FE5C2nB"
      },
      "outputs": [],
      "source": [
        "!python -m tensor2tensor.rl.evaluator \\\n",
        "  --loop_hparams_set=rlmf_tiny \\\n",
        "  --hparams=game=pong \\\n",
        "  --policy_dir=mf_pong \\\n",
        "  --debug_video_path=mf_pong \\\n",
        "  --num_debug_videos=4 \\\n",
        "  --eval_metrics_dir=mf_pong/full_eval_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDoR0C0ZKCOn"
      },
      "outputs": [],
      "source": [
        "play_video('mf_pong/0.avi')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQmZEVKGF4Hh"
      },
      "source": [
        "# Model-based training\n",
        "\n",
        "The `rl` package offers many more features, including model-based training. For instructions on how to use them, go to our [README](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/rl/README.md)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "hello_t2t-rl.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}