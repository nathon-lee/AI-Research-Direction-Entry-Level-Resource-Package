
scholar
Openai的论文已经表示，可以通过让大模型与人进行交互，来提升大模型的性能，这其中的提升方式包括，
1、通过稀疏网络学习关键信息，从而减少训练和推理的时间
2、大模型与人交互，如何让模型感知到重点，从而不断更新和优化模型

关于稀疏
辛顿的蒸馏以及Jeff dean相关的sparse网络都有利于提升网络训练和推理的速度

其实无外乎两种，文本模型和视觉模型，文本模型一般通过transformer训练能够取得较好的性能，视觉模型一般通过difussion可以取得较好的性能。
因为transformer方法在大模型中的极好表现，目前很多论文也开始采用transformer训练视觉模型。

从目前智能眼镜的发展来看，将视觉和语言模型应用在智能眼镜上，将会是一个很好的方向。

在一篇vision transformer的论文中看到，通过视觉学习，模型将学习到更多的内容，从人类学习技能的过程来看，确实是这样。
但两者结合将是一种更好的方式，世界模型可能将会是这两者结合的一个很好产物。


核心的算法都是基于概率， 重点在于如何以较小代价学习到最有价值的内容，从而提升模型的能力。

另外一个好的方向是迁移学习，将在本地学习到的经验迁移到其他地方。再与其他地方的有效经验混合，得到可信度更高的经验。
之前看过一篇提升神经网络训练性能的paper，核心是将之前的单通道修改为多通道，从而提升训练的速度，其实有点类似于将复杂的网络拆分为稀疏的网络，这样每次迭代或者训练的开销都会大大降低。

人类的经验可能和神经网络中的经验有些不同，因为它将人类世界可表征的经验全都抽象成token，浮点数概率一类计算机方便运算的数字符号。
在编码与解码的过程中是否会丢失一些有效的内容还有待验证。

是否最终会有一种模型 - 各大公司的大模型，基于context不同的情况，人们对于同一个事情的观点可能也是不同的
可以从医疗来看，几乎各种疾病的诊断领域，都有很好的医生，因为有区分，所有有不同，更好的医生大概率具备更好的治疗效果，可是为什么具备很好治疗效果的医生也只有少数呢？难道这也服从某种概率分布？这些医生的能力能否复制？

信任阈值，通过概率很好模拟，而世界上几千年来的持续的交易大都建立在信任的基础上。jd个人认为也是因为这个。

认知可信，区块链可信技术与LLM结合

CPU繁忙程度反映 经济活动分布

曾经考试利用艾宾浩斯遗忘曲线来学习知识，相比不断学习新知识，这种方式可以延长知识在大脑中存在的时间。这其实类似于RL，通过不断刺激，增加某一类知识的权重。而人类的大脑记忆模块不过是被事先设计好的生物机器。

Thinkmachine.ai在2025/10月发布了tink，简单就是通过lora进行小范围的调参，从而训练属于每个人的私有模型。
可以大量的结合https://thinkingmachines.ai/blog/ 公开的一些文章 

AI语音 diffusion & transformer

区块链可信算法 和 LLM结合 构建宪法智能体


如果把我们每个人所处的环境看作一个博弈圈, 每个个体所能接收到的多模态信息作为输入, 多模态信息互相交织, 与外界或其它更成熟的博弈圈融合
个体所处的环境信息交由系统分析处理, 做出最有益于当前服务个体的选择.
而哪些信息应该保留, 是否人的一些动作, 神经的异常波动, 就能探测
记忆最近的信息, 用于及时反思, 没被特别标记的信息, 逐步丢失

我们现在所能接触到的工具, 无非都在帮助我们获取我们想要获取的信息, 但这种信息获取是否真的就很高效呢?


父母不是能够完全指引你的人, 老师也不是, 这个世界上也没有某一个人是, 可是谁是呢?

人类自诞生以来, 都是自总结的个体, “他”总结将塑造更好的我们

- 人有遗忘, 人是生物, 我们是不确定的意识存在. 而模型是否可以记住我们所经历的所有?
