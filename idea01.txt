
scholar
Openai的论文已经表示，可以通过让大模型与人进行交互，来提升大模型的性能，这其中的提升方式包括，
1、通过稀疏网络学习关键信息，从而减少训练和推理的时间
2、大模型与人交互，如何让模型感知到重点，从而不断更新和优化模型

关于稀疏
辛顿的蒸馏以及Jeff dean相关的sparse网络都有利于提升网络训练和推理的速度

其实无外乎两种，文本模型和视觉模型，文本模型一般通过transformer训练能够取得较好的性能，视觉模型一般通过difussion可以取得较好的性能。
因为transformer方法在大模型中的极好表现，目前很多论文也开始采用transformer训练视觉模型。

从目前智能眼镜的发展来看，将视觉和语言模型应用在智能眼镜上，将会是一个很好的方向。

在一篇vision transformer的论文中看到，通过视觉学习，模型将学习到更多的内容，从人类学习技能的过程来看，确实是这样。
但两者结合将是一种更好的方式，世界模型可能将会是这两者结合的一个很好产物。


核心的算法都是基于概率， 重点在于如何以较小代价学习到最有价值的内容，从而提升模型的能力。

另外一个好的方向是迁移学习，将在本地学习到的经验迁移到其他地方。再与其他地方的有效经验混合，得到可信度更高的经验。
之前看过一篇提升神经网络训练性能的paper，核心是将之前的单通道修改为多通道，从而提升训练的速度，其实有点类似于将复杂的网络拆分为稀疏的网络，这样每次迭代或者训练的开销都会大大降低。

人类的经验可能和神经网络中的经验有些不同，因为它将人类世界可表征的经验全都抽象成token，浮点数概率一类计算机方便运算的数字符号。
在编码与解码的过程中是否会丢失一些有效的内容还有待验证。

是否最终会有一种模型 - 各大公司的大模型，基于context不同的情况，人们对于同一个事情的观点可能也是不同的
可以从医疗来看，几乎各种疾病的诊断领域，都有很好的医生，因为有区分，所有有不同，更好的医生大概率具备更好的治疗效果，可是为什么具备很好治疗效果的医生也只有少数呢？难道这也服从某种概率分布？这些医生的能力能否复制？

信任阈值，通过概率很好模拟，而世界上几千年来的持续的交易大都建立在信任的基础上。jd个人认为也是因为这个。

认知可信，区块链可信技术与LLM结合

CPU繁忙程度反映 经济活动分布

曾经考试利用艾宾浩斯遗忘曲线来学习知识，相比不断学习新知识，这种方式可以延长知识在大脑中存在的时间。这其实类似于RL，通过不断刺激，增加某一类知识的权重。而人类的大脑记忆模块不过是被事先设计好的生物机器。

Thinkmachine.ai在2025/10月发布了tink，简单就是通过lora进行小范围的调参，从而训练属于每个人的私有模型。
可以大量的结合https://thinkingmachines.ai/blog/ 公开的一些文章 

AI语音 diffusion & transformer

区块链可信算法 和 LLM结合 构建宪法智能体
